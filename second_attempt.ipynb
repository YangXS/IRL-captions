{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Dim: 1004\n",
      "Image Feature Dim: 4096\n",
      "Word Embedding Dim: 304\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import layer_utils\n",
    "from coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from image_utils import image_from_url\n",
    "from lstm import LSTM\n",
    "\n",
    "def show_image(batch_size):\n",
    "    captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "    for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "        plt.imshow(image_from_url(url))\n",
    "        plt.axis('off')\n",
    "        caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "        plt.title(caption_str)\n",
    "        plt.show()\n",
    "        \n",
    "def sparse_to_one_hot(sparse_input, max_dim):\n",
    "    one_hot = np.zeros((sparse_input.shape[0], max_dim))\n",
    "    for idx, input_index in enumerate(sparse_input):\n",
    "        one_hot[idx, input_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "def captions_to_one_hot(captions, vocab_dim):\n",
    "    return [sparse_to_one_hot(sentence, vocab_dim) for sentence in captions]\n",
    "\n",
    "def verify_caption_train_target_offset(train_caption, target_caption):\n",
    "    for i in range(len(target_caption) - 1):\n",
    "        assert train_caption[i + 1] == target_caption[i]\n",
    "        \n",
    "def get_train_target_caption(train_captions_as_word_ids, null_representation):\n",
    "    \"\"\"\n",
    "        Convert training data:\n",
    "        '<START> a variety of fruits and vegetables sitting on a kitchen counter'\n",
    "        to target:\n",
    "        'a variety of fruits and vegetables sitting on a kitchen counter <END>'\n",
    "    \"\"\"\n",
    "    target_captions_as_word_ids = train_captions_as_word_ids[:, 1:]\n",
    "    train_captions_as_word_ids = train_captions_as_word_ids[:, :-1]\n",
    "    verify_caption_train_target_offset(train_captions_as_word_ids[0], target_captions_as_word_ids[0])\n",
    "    not_null_target_mask = target_captions_as_word_ids != null_representation\n",
    "    return train_captions_as_word_ids, target_captions_as_word_ids, not_null_target_mask\n",
    "\n",
    "# Load Data\n",
    "data = load_coco_data(pca_features=False)\n",
    "\n",
    "## word preprocess\n",
    "vocab_dim = len(data['word_to_idx'])\n",
    "image_feature_dim = data['val_features'].shape[1]\n",
    "enable_preprocessed_embedding = True\n",
    "\n",
    "if enable_preprocessed_embedding:\n",
    "    word_embedding_dim = data['word_embedding'].shape[1]\n",
    "else:\n",
    "    word_embedding_dim = 256\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "NULL_TOKEN = '<NULL>'\n",
    "NULL_ID = data['word_to_idx'][NULL_TOKEN]\n",
    "START_ID = data['word_to_idx'][START_TOKEN]\n",
    "END_ID = data['word_to_idx'][END_TOKEN]\n",
    "\n",
    "print(\"Vocab Dim: %i\\nImage Feature Dim: %i\\nWord Embedding Dim: %i\"%(vocab_dim,\n",
    "                                                                      image_feature_dim,\n",
    "                                                                      word_embedding_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(50, ?, 304), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "batch_size = 50\n",
    "lstm = LSTM(hidden_dim=hidden_dim,\n",
    "            output_dim=vocab_dim,\n",
    "            learning_rate=5e-3,\n",
    "            batch_size=batch_size,\n",
    "            num_layers=1)\n",
    "\n",
    "# Word Input\n",
    "sy_caption_input = tf.placeholder(shape=[batch_size, None], name=\"caption_input\", dtype=tf.int32)\n",
    "\n",
    "if enable_preprocessed_embedding:\n",
    "    embedding_init = tf.constant(data['word_embedding'], dtype=tf.float32)\n",
    "    embedding = tf.get_variable(\"embedding\", initializer=embedding_init)\n",
    "else:\n",
    "    embedding_init = tf.random_normal_initializer()\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_dim, word_embedding_dim], dtype=tf.float32, initializer=embedding_init)\n",
    "\n",
    "word_embedding = tf.nn.embedding_lookup(embedding, sy_caption_input)\n",
    "print(word_embedding)\n",
    "\n",
    "# Image Input\n",
    "sy_image_feat_input = tf.placeholder(shape=[batch_size, image_feature_dim], name=\"image_feat_input\", dtype=tf.float32)\n",
    "initial_hidden_state = layer_utils.affine_transform(sy_image_feat_input, hidden_dim, 'image_proj')\n",
    "initial_cell_state = initial_hidden_state * 0\n",
    "\n",
    "lstm.build_model(word_embedding, initial_hidden_state, initial_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, cross-entropy: 110.65420532226562, accuracy: 0.0\n",
      "iter 10, cross-entropy: 55.786048889160156, accuracy: 0.15535713732242584\n",
      "iter 20, cross-entropy: 49.351375579833984, accuracy: 0.2935943007469177\n",
      "iter 30, cross-entropy: 45.969852447509766, accuracy: 0.307823121547699\n",
      "iter 40, cross-entropy: 44.595619201660156, accuracy: 0.3239929974079132\n",
      "iter 50, cross-entropy: 40.486454010009766, accuracy: 0.3545454442501068\n",
      "iter 60, cross-entropy: 39.169620513916016, accuracy: 0.3314606845378876\n",
      "iter 70, cross-entropy: 37.287044525146484, accuracy: 0.3502722382545471\n",
      "iter 80, cross-entropy: 38.09480285644531, accuracy: 0.3571428656578064\n",
      "iter 90, cross-entropy: 35.696754455566406, accuracy: 0.35769230127334595\n",
      "iter 100, cross-entropy: 33.95124053955078, accuracy: 0.3696857690811157\n",
      "iter 110, cross-entropy: 36.169403076171875, accuracy: 0.34285715222358704\n",
      "iter 120, cross-entropy: 36.18818283081055, accuracy: 0.34727272391319275\n",
      "iter 130, cross-entropy: 36.036197662353516, accuracy: 0.3543165326118469\n",
      "iter 140, cross-entropy: 33.980831146240234, accuracy: 0.394316166639328\n",
      "iter 150, cross-entropy: 35.40581130981445, accuracy: 0.37745973467826843\n",
      "iter 160, cross-entropy: 32.53038024902344, accuracy: 0.3756805956363678\n",
      "iter 170, cross-entropy: 31.401697158813477, accuracy: 0.4397905766963959\n",
      "iter 180, cross-entropy: 32.27609634399414, accuracy: 0.4341636896133423\n",
      "iter 190, cross-entropy: 33.529571533203125, accuracy: 0.37842777371406555\n"
     ]
    }
   ],
   "source": [
    "iter_num = 200\n",
    "embeddings = []\n",
    "for i in range(iter_num):\n",
    "    mini_batch, features, url = sample_coco_minibatch(data,  batch_size=batch_size, split='train')\n",
    "    train_captions, target_captions, target_mask = get_train_target_caption(mini_batch, NULL_ID)\n",
    "    feed_dict = {\n",
    "        sy_caption_input: train_captions,\n",
    "        sy_image_feat_input: features\n",
    "    }\n",
    "    c, a = lstm.train(sess, target_captions, target_mask, feed_dict)\n",
    "    if (i % 10 == 0):\n",
    "        print(\"iter {}, cross-entropy: {}, accuracy: {}\".format(i, c, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4   3 125  11 230  32   7  25   2   0   0   0   0   0   0   0]\n",
      "[[ 17.6590004  -10.06469536   6.91883516 ...,  -9.64595318  -8.3009491\n",
      "   -6.36691332]]\n",
      "GT:an older <UNK> <UNK> truck with the truck bed covered by a <UNK> <END>\n",
      "Test:a <UNK> truck is driving down the street <END>\n"
     ]
    }
   ],
   "source": [
    "def decode_caption_with(word_id_sequence, key_name = 'idx_to_word'):\n",
    "    id_to_word = data[key_name]\n",
    "    return decode_captions(word_id_sequence, id_to_word)\n",
    "\n",
    "mini_batch, features, url = sample_coco_minibatch(data,  batch_size=batch_size, split='val')\n",
    "trash, GT_captions, GT_mask = get_train_target_caption(mini_batch, NULL_ID)\n",
    "test_input = np.ones((batch_size, 1)) * START_ID\n",
    "feed_dict = {\n",
    "    sy_caption_input: test_input,\n",
    "    sy_image_feat_input: features\n",
    "}\n",
    "output, logits = lstm.test(sess, sy_caption_input, feed_dict)\n",
    "\n",
    "print(output[0])\n",
    "print(logits[0])\n",
    "print(\"GT:{}\".format(decode_caption_with(GT_captions[0])))\n",
    "print(\"Test:{}\".format(decode_caption_with(output[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

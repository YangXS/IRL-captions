{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Dim: 1004\n",
      "Image Feature Dim: 4096\n",
      "Word Embedding Dim: 304\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import layer_utils\n",
    "from coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from image_utils import image_from_url\n",
    "from lstm import LSTM\n",
    "\n",
    "def show_image(batch_size):\n",
    "    captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "    for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "        plt.imshow(image_from_url(url))\n",
    "        plt.axis('off')\n",
    "        caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "        plt.title(caption_str)\n",
    "        plt.show()\n",
    "        \n",
    "def sparse_to_one_hot(sparse_input, max_dim):\n",
    "    one_hot = np.zeros((sparse_input.shape[0], max_dim))\n",
    "    for idx, input_index in enumerate(sparse_input):\n",
    "        one_hot[idx, input_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "def captions_to_one_hot(captions, vocab_dim):\n",
    "    return [sparse_to_one_hot(sentence, vocab_dim) for sentence in captions]\n",
    "\n",
    "def verify_caption_train_target_offset(train_caption, target_caption):\n",
    "    for i in range(len(target_caption) - 1):\n",
    "        assert train_caption[i + 1] == target_caption[i]\n",
    "        \n",
    "def get_train_target_caption(train_captions_as_word_ids, null_representation):\n",
    "    \"\"\"\n",
    "        Convert training data:\n",
    "        '<START> a variety of fruits and vegetables sitting on a kitchen counter'\n",
    "        to target:\n",
    "        'a variety of fruits and vegetables sitting on a kitchen counter <END>'\n",
    "    \"\"\"\n",
    "    target_captions_as_word_ids = train_captions_as_word_ids[:, 1:]\n",
    "    train_captions_as_word_ids = train_captions_as_word_ids[:, :-1]\n",
    "    verify_caption_train_target_offset(train_captions_as_word_ids[0], target_captions_as_word_ids[0])\n",
    "    not_null_target_mask = target_captions_as_word_ids != null_representation\n",
    "    return train_captions_as_word_ids, target_captions_as_word_ids, not_null_target_mask\n",
    "\n",
    "# Load Data\n",
    "data = load_coco_data(pca_features=False)\n",
    "\n",
    "## word preprocess\n",
    "vocab_dim = len(data['word_to_idx'])\n",
    "image_feature_dim = data['val_features'].shape[1]\n",
    "enable_preprocessed_embedding = True\n",
    "\n",
    "if enable_preprocessed_embedding:\n",
    "    word_embedding_dim = data['word_embedding'].shape[1]\n",
    "else:\n",
    "    word_embedding_dim = 256\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "NULL_TOKEN = '<NULL>'\n",
    "NULL_ID = data['word_to_idx'][NULL_TOKEN]\n",
    "START_ID = data['word_to_idx'][START_TOKEN]\n",
    "END_ID = data['word_to_idx'][END_TOKEN]\n",
    "\n",
    "print(\"Vocab Dim: %i\\nImage Feature Dim: %i\\nWord Embedding Dim: %i\"%(vocab_dim,\n",
    "                                                                      image_feature_dim,\n",
    "                                                                      word_embedding_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(50, ?, 304), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "batch_size = 50\n",
    "lstm = LSTM(hidden_dim=hidden_dim,\n",
    "            output_dim=vocab_dim,\n",
    "            learning_rate=5e-3,\n",
    "            batch_size=batch_size,\n",
    "            num_layers=1)\n",
    "\n",
    "# Word Input\n",
    "sy_caption_input = tf.placeholder(shape=[batch_size, None], name=\"caption_input\", dtype=tf.int32)\n",
    "\n",
    "if enable_preprocessed_embedding:\n",
    "    embedding_init = tf.constant(data['word_embedding'], dtype=tf.float32)\n",
    "    embedding = tf.get_variable(\"embedding\", initializer=embedding_init)\n",
    "else:\n",
    "    embedding_init = tf.random_normal_initializer()\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_dim, word_embedding_dim], dtype=tf.float32, initializer=embedding_init)\n",
    "\n",
    "word_embedding = tf.nn.embedding_lookup(embedding, sy_caption_input)\n",
    "print(word_embedding)\n",
    "\n",
    "# Image Input\n",
    "sy_image_feat_input = tf.placeholder(shape=[batch_size, image_feature_dim], name=\"image_feat_input\", dtype=tf.float32)\n",
    "initial_hidden_state = layer_utils.affine_transform(sy_image_feat_input, hidden_dim, 'image_proj')\n",
    "initial_cell_state = initial_hidden_state * 0\n",
    "\n",
    "lstm.build_model(word_embedding, initial_hidden_state, initial_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, cross-entropy: 35.00442123413086, accuracy: 0.38235294818878174\n",
      "iter 10, cross-entropy: 32.651302337646484, accuracy: 0.4020797312259674\n",
      "iter 20, cross-entropy: 34.308109283447266, accuracy: 0.3986014127731323\n",
      "iter 30, cross-entropy: 31.362855911254883, accuracy: 0.3955223858356476\n",
      "iter 40, cross-entropy: 32.74540710449219, accuracy: 0.4060283601284027\n",
      "iter 50, cross-entropy: 33.60277557373047, accuracy: 0.3986254334449768\n",
      "iter 60, cross-entropy: 31.882848739624023, accuracy: 0.4003496468067169\n",
      "iter 70, cross-entropy: 31.202634811401367, accuracy: 0.37833037972450256\n",
      "iter 80, cross-entropy: 31.555641174316406, accuracy: 0.3974591791629791\n",
      "iter 90, cross-entropy: 32.09532165527344, accuracy: 0.3731343150138855\n",
      "iter 100, cross-entropy: 31.986225128173828, accuracy: 0.3759259283542633\n",
      "iter 110, cross-entropy: 29.608427047729492, accuracy: 0.41360294818878174\n",
      "iter 120, cross-entropy: 30.509275436401367, accuracy: 0.40250447392463684\n",
      "iter 130, cross-entropy: 30.927257537841797, accuracy: 0.38998210430145264\n",
      "iter 140, cross-entropy: 29.399085998535156, accuracy: 0.3903743326663971\n",
      "iter 150, cross-entropy: 30.729324340820312, accuracy: 0.4163636267185211\n",
      "iter 160, cross-entropy: 28.898792266845703, accuracy: 0.4165137708187103\n",
      "iter 170, cross-entropy: 28.441999435424805, accuracy: 0.4275229275226593\n",
      "iter 180, cross-entropy: 33.06217956542969, accuracy: 0.40070298314094543\n",
      "iter 190, cross-entropy: 31.75274085998535, accuracy: 0.394316166639328\n"
     ]
    }
   ],
   "source": [
    "iter_num = 200\n",
    "embeddings = []\n",
    "for i in range(iter_num):\n",
    "    mini_batch, features, url = sample_coco_minibatch(data,  batch_size=batch_size, split='train')\n",
    "    train_captions, target_captions, target_mask = get_train_target_caption(mini_batch, NULL_ID)\n",
    "    feed_dict = {\n",
    "        sy_caption_input: train_captions,\n",
    "        sy_image_feat_input: features\n",
    "    }\n",
    "    c, a = lstm.train(sess, target_captions, target_mask, feed_dict)\n",
    "    if (i % 10 == 0):\n",
    "        print(\"iter {}, cross-entropy: {}, accuracy: {}\".format(i, c, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  29  91 217   9   4  91   5   7  69   6  31   2   0   0   0]\n",
      "[[ 18.11688995  -7.46315861   5.94842863 ...,  -6.25985098  -5.1451869\n",
      "   -4.78702831]]\n",
      "(50, 16)\n",
      "GT:a large tall tower with a clock on top <END>\n",
      "Test:a large clock tower with a clock on the side of it <END>\n",
      "Pseudo-Test:a large clock tower with a clock on the of\n"
     ]
    }
   ],
   "source": [
    "def decode_caption_with(word_id_sequence, key_name = 'idx_to_word'):\n",
    "    id_to_word = data[key_name]\n",
    "    return decode_captions(word_id_sequence, id_to_word)\n",
    "\n",
    "mini_batch, features, url = sample_coco_minibatch(data,  batch_size=batch_size, split='val')\n",
    "GT_input, GT_captions, GT_mask = get_train_target_caption(mini_batch, NULL_ID)\n",
    "test_input = np.ones((batch_size, 1)) * START_ID\n",
    "feed_dict = {\n",
    "    sy_caption_input: test_input,\n",
    "    sy_image_feat_input: features\n",
    "}\n",
    "output, logits = lstm.test(sess, sy_caption_input, feed_dict)\n",
    "\n",
    "feed_dict[sy_caption_input] = GT_input\n",
    "pseudo_output = lstm.pseudo_test(sess, GT_captions, feed_dict)\n",
    "\n",
    "\n",
    "print(\"GT:{}\".format(decode_caption_with(GT_captions[0])))\n",
    "print(\"Test:{}\".format(decode_caption_with(output[0])))\n",
    "print(\"Pseudo-Test:{}\".format(decode_caption_with(pseudo_output[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

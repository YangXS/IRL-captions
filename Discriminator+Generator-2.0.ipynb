{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminator.discriminator_wrapper import DiscriminatorWrapper\n",
    "from coco_utils import load_coco_data_struct\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from generator.generator_wrapper import GeneratorWrapper, GeneratorSpec\n",
    "from generator.generator_data import GeneratorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sess():\n",
    "    global sess\n",
    "    ruv = set(sess.run(tf.report_uninitialized_variables()))\n",
    "    uv = [v for v in tf.global_variables() if v.name.split(':')[0].encode('ascii') in ruv]\n",
    "    tf.variables_initializer(uv).run()\n",
    "    \n",
    "def reset_sess():\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "def get_mean_reward(rewards):\n",
    "    np_rewards = np.array(rewards)\n",
    "    rows, columns = np.nonzero(np_rewards)\n",
    "    indices = [i - 1 for i, val in enumerate(rows) if i > 0 and val != rows[i-1] or i == rows.shape[0]-1]\n",
    "    final_rewards = np.zeros((len(indices)))\n",
    "    for i, idx in enumerate(indices):\n",
    "        final_rewards[i] = np_rewards[rows[idx], columns[idx]]\n",
    "    return np.mean(final_rewards)\n",
    "    \n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded vocab data.\n",
      "Embedding <class 'numpy.ndarray'> (1004, 304) float64\n",
      "Word to index <class 'dict'> 1004\n",
      "Index to word <class 'list'> 1004\n",
      "\n",
      "Loaded train data.\n",
      "Captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "Image indices <class 'numpy.ndarray'> (400135,) int32\n",
      "Image features <class 'numpy.ndarray'> (82783, 4096) float32\n",
      "Image urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "\n",
      "Loaded val data.\n",
      "Captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "Image indices <class 'numpy.ndarray'> (195954,) int32\n",
      "Image features <class 'numpy.ndarray'> (40504, 4096) float32\n",
      "Image urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "vocab_data, train_data, val_data = load_coco_data_struct()\n",
    "gendata = GeneratorData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain\n",
    "Only run this section to pretrain the network again. This section will save the model which can then be loaded to immediately start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_iterations = 3\n",
    "mle_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_spec = GeneratorSpec(input_dim=512, hidden_dim=512, output_dim=1004, rnn_activation=None,\n",
    "                         image_feature_dim=4096, n_seq_steps=16,\n",
    "                         embedding_init=tf.constant(gendata.word_embedding, dtype=tf.float32),\n",
    "                         n_baseline_layers=2, baseline_hidden_dim=64,\n",
    "                         mle_learning_rate=5e-3, pg_learning_rate=5e-4,\n",
    "                         baseline_learning_rate=5e-3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 5.122125230627717e-07\n"
     ]
    }
   ],
   "source": [
    "disc = DiscriminatorWrapper(train_data, val_data, vocab_data)\n",
    "initialize_sess()\n",
    "\n",
    "train_loss, val_loss = disc.pre_train(sess, iter_num=disc_iterations, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, cross-entropy: 110.73542022705078, accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "gen = GeneratorWrapper(gen_spec, disc.assign_reward)\n",
    "initialize_sess()\n",
    "\n",
    "cross_entropy, accuracy = gen.train(sess, gendata, num_iterations=mle_iterations, training_type='MLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model into a models/ folder which is excluded from git using the .gitignore\n",
    "gen.save(sess, \"pretrained-mle%d-disc%d\" % (mle_iterations, disc_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "Load an existing model that contains both a discrminator and generator (policy gradient) network. If the model has contained a policy gradient LSTM before, then set is_PG to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained-mle3-disc3\n"
     ]
    }
   ],
   "source": [
    "dir_name=\"models\"\n",
    "model_name = \"pretrained-mle%d-disc%d\" % (mle_iterations, disc_iterations)\n",
    "\n",
    "# Load discriminator\n",
    "disc = DiscriminatorWrapper(train_data, val_data, vocab_data, load_session=sess,\n",
    "                            saved_model_name=model_name, model_base_dir=dir_name)\n",
    "\n",
    "# Load generator\n",
    "gen = GeneratorWrapper(gen_spec, disc.assign_reward, True)\n",
    "\n",
    "initialize_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Alternate between running the two cells below or enclose their contents in a loop. The former is probably wiser since sometimes either side will require fewer or more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_iterations = 5\n",
    "good_seq_req = 3\n",
    "good_seq = 0\n",
    "captions = []\n",
    "probs = []\n",
    "indexes = []\n",
    "rewards = []\n",
    "\n",
    "while pg_iterations > 0 and good_seq < good_seq_req:\n",
    "    caption, prob, index, reward = gen.train(sess, gendata.shuffle())\n",
    "    mean_reward = get_mean_reward(reward)\n",
    "    if mean_reward > 0.8 + cycle * 0.001:\n",
    "        captions.extend(caption)\n",
    "        probs.extend(prob)\n",
    "        indexes.extend(index)\n",
    "        rewards.extend(reward)\n",
    "        pg_iterations -= 1\n",
    "        good_seq += 1\n",
    "    else:\n",
    "        good_seq = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_iterations = 120\n",
    "online_all_loss, online_val_loss = disc.online_train(sess, iter_num=disc_iterations, img_idxs=np.array(indexes),\n",
    "                                                     caption_sentences=captions)\n",
    "\n",
    "disc.assign_reward(sess, np.array(indexes)[0:1], captions[0:1], image_idx_from_training=True, to_examine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save_model(sess, \"full-discriminator-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

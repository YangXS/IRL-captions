{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from discriminator.discriminator_wrapper import DiscriminatorWrapper\n",
    "from coco_utils import load_coco_data_struct\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded vocab data.\n",
      "Embedding <class 'numpy.ndarray'> (1004, 304) float64\n",
      "Word to index <class 'dict'> 1004\n",
      "Index to word <class 'list'> 1004\n",
      "\n",
      "Loaded train data.\n",
      "Captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "Image indices <class 'numpy.ndarray'> (400135,) int32\n",
      "Image features <class 'numpy.ndarray'> (82783, 4096) float32\n",
      "Image urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "\n",
      "Loaded val data.\n",
      "Captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "Image indices <class 'numpy.ndarray'> (195954,) int32\n",
      "Image features <class 'numpy.ndarray'> (40504, 4096) float32\n",
      "Image urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "vocab_data, train_data, val_data = load_coco_data_struct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 400\n",
    "disc = DiscriminatorWrapper(train_data, val_data, vocab_data)\n",
    "train_loss, val_loss = disc.pre_train(sess, iter_num=iter_num, batch_size=1000)\n",
    "assert len(train_loss) == len(val_loss)\n",
    "assert len(train_loss) == iter_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_idx = np.array([1, 2, 3])\n",
    "captions = np.array([\"haha hehe\", \"hello wordl\", \"cat dogs one helloo\"])\n",
    "_, rewards, _ = disc.assign_reward(sess, train_image_idx, captions,\n",
    "                                   image_idx_from_training=True, to_examine=False)\n",
    "assert rewards.shape[0] == len(train_image_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: -0.5205084681510925\n"
     ]
    }
   ],
   "source": [
    "online_all_loss, online_val_loss = disc.online_train(sess, iter_num=20, img_idxs=train_image_idx, caption_sentences=captions)\n",
    "\n",
    "assert len(online_all_loss) == len(online_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
